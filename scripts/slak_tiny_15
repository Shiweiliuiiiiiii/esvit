#!/bin/bash
#SBATCH --job-name=esvit_slak_15x15_bn
#SBATCH -p gpu
#SBATCH -N 1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=4
#SBATCH --gpus=4
#SBATCH -t 3-11:59:59
#SBATCH --exclusive
#SBATCH --cpus-per-task=72
#SBATCH -o esvit_slak_15x15_bn.out

source /home/sliu/miniconda3/etc/profile.d/conda.sh
source activate LoRA


python -m torch.distributed.launch --nproc_per_node=4 main_esvit.py \
--arch SLaK_tiny --kernel_size 15 15 15 15 100 --bn True  \
--data_path /projects/2/managed_datasets/imagenet/train/ \
--output_dir ~/project_space/esvit/output/esvit_exp/ssl/slak_tiny_imagenet_15x15/ \
--batch_size_per_gpu 128 --epochs 300 --teacher_temp 0.07 --warmup_epochs 10 \
--warmup_teacher_temp_epochs 30 --norm_last_layer false --use_dense_prediction True \
--cfg experiments/imagenet/swin/swin_tiny_patch4_window7_224.yaml

 source deactivate